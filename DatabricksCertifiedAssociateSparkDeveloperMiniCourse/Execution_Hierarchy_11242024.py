# Databricks notebook source
# MAGIC %run ./DCAD_Training/Create_DataFrames_pyspark

# COMMAND ----------

import pyspark.sql.functions as F

# COMMAND ----------

df = customerDf \
    .join(addressDf,
          customerDf["address_id"] == addressDf["address_id"]) \
    .filter(F.year(F.col("birthdate")) > 1980) \
    .groupBy(customerDf["demographics.education_status"], addressDf["location_type"]) \
    .agg(F.countDistinct("customer_id").alias("count")) \
    .where("location_type is not null") \
    .sort(F.desc("count"))

# See all the of the plans
df.explain(True)

# COMMAND ----------

# View the source code that was generated by Apache SPark
df.explain("codegen")

# COMMAND ----------

df.show()

# COMMAND ----------

# Get the number of partitions
spark.conf.get("spark.sql.shuffle.partitions")

# COMMAND ----------

# Get the number of partitions of a particular dataframe
customerDf.rdd.getNumPartitions()

# COMMAND ----------

# Set default num partitions
spark.conf.set("spark.sql.shuffle.partitions", 50)

# COMMAND ----------

# Changing the number of partitions for a dataframe
customerDf.repartition(2).rdd.getNumPartitions() # This will change the execution plan and runtime of the above dataframe generation

# COMMAND ----------

# Changing the number of partitions using coalesce
addressDf.coalesce(5).rdd.getNumPartitions() # Will only work when shrinking the number of partitions

# COMMAND ----------


